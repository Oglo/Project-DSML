{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%204/Assignment_part_four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression**\n",
    "\n",
    "In our project, we initially opted for Logistic Regression due to its reputation as a reliable and efficient model for various classification tasks. Recognizing that it might not be the ideal fit for the nuanced challenge of classifying text difficulty levels, we were still intrigued to see its performance. \n",
    "\n",
    "Our choice was partly driven by the desire to establish a baseline with a well-understood and widely-used model, even though we were conscious of its potential limitations in handling the complexities inherent in textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/training_data.csv\").dropna()\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/unlabelled_test_data.csv\").dropna()\n",
    "df_final = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/sample_submission.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(df_train[['difficulty']])\n",
    "\n",
    "X = df_train['sentence']\n",
    "y = y_encoded\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_train['difficulty'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs'))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = pipeline.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "y_pred_test = pipeline.predict(df_test['sentence'])\n",
    "y_pred_test_labels = label_encoder.inverse_transform(y_pred_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Premières prédictions de test:\", y_pred_test_labels[:5])\n",
    "\n",
    "df_final['difficulty'] = y_pred_test\n",
    "difficulty_mapping = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}\n",
    "\n",
    "df_final['difficulty'] = df_final['difficulty'].map(difficulty_mapping)\n",
    "df_final"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
