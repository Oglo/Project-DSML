{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1cP2OoW75gnQF7nYUCwHNly-OG6xA_1Gi#scrollTo=7zENRfQhl1h4\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT**\n",
    "\n",
    "In this new attempt, we employed BERT (Bidirectional Encoder Representations from Transformers), a cutting-edge model in natural language processing, combined with a classification layer. BERT is renowned for its ability to deeply understand the context and nuances of language, making it highly suitable for complex tasks like text difficulty classification. We preprocessed our text data to fit BERT’s requirements, involving lowercasing, removing punctuation and numbers, and condensing spaces.\n",
    "\n",
    "Then, we used BERT’s tokenizer to convert our sentences into a format understandable by the model, followed by padding these tokenized sentences to a fixed length for consistent input size. We ran this data through the BERT model, which is adept at extracting meaningful features from text, and then used these features for classification into different difficulty levels. Training BERT required careful tuning of parameters and understanding its learning process, but its advanced understanding of language context offered us a significant advantage in accurately classifying text difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/training_data.csv\").dropna()\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/unlabelled_test_data.csv\").dropna()\n",
    "df_final = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/sample_submission.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    return text\n",
    "\n",
    "def encode_sentences(tokenizer, sentences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "training_data = df_train\n",
    "training_data['cleaned_sentence'] = training_data['sentence'].apply(preprocess_text)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=6)\n",
    "model.to(device)\n",
    "\n",
    "max_length = 256  \n",
    "\n",
    "train_inputs, train_masks = encode_sentences(tokenizer, training_data['cleaned_sentence'], max_length)\n",
    "train_labels = torch.tensor(training_data['difficulty'].factorize()[0])  # Convertir en indices numériques\n",
    "\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "epochs = 4 \n",
    "lr = 3e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_input_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} - Loss: {avg_train_loss}')\n",
    "\n",
    "unlabelled_data = df_test\n",
    "unlabelled_data['cleaned_sentence'] = unlabelled_data['sentence'].apply(preprocess_text)\n",
    "test_inputs, test_masks = encode_sentences(tokenizer, unlabelled_data['cleaned_sentence'], max_length)\n",
    "test_data = TensorDataset(test_inputs, test_masks)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "all_logits = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_input_ids = batch[0].to(device)\n",
    "    batch_input_mask = batch[1].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask)\n",
    "    logits = outputs[0]\n",
    "    all_logits.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "predicted_labels = np.argmax(all_logits, axis=1)\n",
    "label_mapping = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': unlabelled_data['id'],\n",
    "    'predicted_difficulty': predicted_labels\n",
    "})\n",
    "predictions_df['predicted_difficulty'] = predictions_df['predicted_difficulty'].map(label_mapping)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
