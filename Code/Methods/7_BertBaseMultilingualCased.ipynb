{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1Lze6KKZxVwH_NLaU1opYwwEHsEks5EvG#scrollTo=i9AvFl1FmLER\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT BASE MULTILINGUAL CASED**\n",
    "\n",
    "In our text classification project, we used the BERT (Bidirectional Encoder Representations from Transformers) model, specifically its 'bert-base-multilingual-cased' version, renowned for its advanced natural language processing capabilities. This model processes text by creating rich, contextualized embeddings, capturing the intricate nuances of language. We further enhanced this model by adding a neural network layer, including a dense layer for complexity and a dropout layer to prevent overfitting, culminating in a softmax layer for classifying texts into different difficulty levels.\n",
    "\n",
    "This setup, combined with TensorFlow's functionalities, leveraged BERT's powerful language understanding with additional neural network layers tailored to our classification needs. The model was optimized using the Adam optimizer and trained in smaller batch sizes to effectively handle the computational demands of processing complex linguistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/training_data.csv\").dropna()\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/unlabelled_test_data.csv\").dropna()\n",
    "df_final = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/sample_submission.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def encode_sentences(sentences, tokenizer, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    return np.array(tf.squeeze(input_ids)), np.array(tf.squeeze(attention_masks))\n",
    "\n",
    "training_data = df_train\n",
    "unlabelled_test_data = df_test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    training_data['sentence'],\n",
    "    training_data['difficulty'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "X_train_ids, X_train_masks = encode_sentences(X_train, tokenizer)\n",
    "X_test_ids, X_test_masks = encode_sentences(X_test, tokenizer)\n",
    "\n",
    "input_ids = Input(shape=(128,), dtype=tf.int32)\n",
    "input_mask = Input(shape=(128,), dtype=tf.int32)\n",
    "embeddings = bert_model(input_ids, attention_mask=input_mask)[0][:, 0, :]\n",
    "x = Dense(64, activation='relu')(embeddings)\n",
    "x = Dropout(0.1)(x)\n",
    "output = Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "model.compile(Adam(lr=2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit([X_train_ids, X_train_masks], y_train_encoded, epochs=3, batch_size=16) \n",
    "\n",
    "model.evaluate([X_test_ids, X_test_masks], y_test_encoded)\n",
    "\n",
    "unlabelled_ids, unlabelled_masks = encode_sentences(unlabelled_test_data['sentence'], tokenizer)\n",
    "predictions = model.predict([unlabelled_ids, unlabelled_masks])\n",
    "predicted_levels = label_encoder.inverse_transform(np.argmax(predictions, axis=1)) \n",
    "\n",
    "unlabelled_test_data['difficulty'] = predicted_levels"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
