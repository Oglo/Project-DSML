{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1s846jzD2EeppSBldjnh7UUrMhXRMpBIP#scrollTo=xA850ZrbKfIL\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlauBERT\n",
    "\n",
    "Firstly, the code begins by importing necessary libraries like Pandas for data handling, PyTorch for deep learning, and Transformers for accessing pretrained language models. Essential functions for evaluation metrics such as precision, recall, and f1-score are also imported.\n",
    "\n",
    "The data preparation phase involves mapping language levels to numeric labels and vice versa, which is crucial for the machine learning model to understand the outputs. The custom TextDataset class processes text data, tokenizing and converting it to PyTorch tensors, which are suitable for input into the FlauBERT model.\n",
    "\n",
    "FlauBERT is chosen for its specialization in the French language. Unlike generic models, FlauBERT has been pre-trained on a diverse set of French texts, making it adept at understanding the nuances and context of the language. This pre-training allows it to effectively grasp various styles and levels of French, which is critical for your project.\n",
    "\n",
    "In the training phase, DataLoaders are used for feeding data in batches. AdamW optimizer is employed along with a learning rate scheduler for effective training. During each epoch, the model is trained and validated, with performance metrics like loss and accuracy being computed and displayed.\n",
    "\n",
    "For predictions, a function is defined to process new text inputs and generate predictions using the trained model. The model is then applied to test data, and the predictions are saved into a CSV file. Finally, the model is saved for future use, avoiding the need for retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/training_data_2.csv\").dropna()\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/unlabelled_test_data.csv\").dropna()\n",
    "df_final = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/sample_submission.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import FlaubertTokenizer, FlaubertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import string\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import sentencepiece as spm\n",
    "import joblib\n",
    "\n",
    "label_map = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "def convert_labels_to_numeric(df, label_column):\n",
    "    df[label_column] = df[label_column].map(label_map)\n",
    "    return df\n",
    "\n",
    "def convert_numeric_to_labels(labels):\n",
    "    return [reverse_label_map[label] for label in labels]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, device, learning_rate=2e-5, epochs=3):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss}')\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            total_eval_accuracy += (predictions == labels).sum().item()\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(val_dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Validation Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "def predict(texts, tokenizer, model, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probs = outputs[0].softmax(1)\n",
    "        predictions.append(probs.argmax().item())\n",
    "    return predictions\n",
    "\n",
    "training_data = convert_labels_to_numeric(df_train, 'difficulty')\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(training_data['sentence'], training_data['difficulty'], test_size=0.1)\n",
    "train_texts.reset_index(drop=True, inplace=True)\n",
    "train_labels.reset_index(drop=True, inplace=True)\n",
    "val_texts.reset_index(drop=True, inplace=True)\n",
    "val_labels.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
    "model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=6)\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_model(model, train_dataset, val_dataset, device)\n",
    "\n",
    "test_texts = df_test['sentence']\n",
    "predicted_difficulties_numeric = predict(test_texts, tokenizer, model, device)\n",
    "predicted_difficulties = convert_numeric_to_labels(predicted_difficulties_numeric)\n",
    "\n",
    "result_df = pd.DataFrame({'id': df_test['id'], 'difficulty': predicted_difficulties})\n",
    "result_df.to_csv('predicted_difficultiesCamenBERT.csv', index=False)\n",
    "\n",
    "torch.save(model.state_dict(), 'FlauBERT.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
