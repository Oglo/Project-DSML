{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%204/Assignment_part_four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT for sequence classification**\n",
    "\n",
    "Finally, we utilized the BertForSequenceClassification model from the BERT (Bidirectional Encoder Representations from Transformers) series, specifically the 'bert-base-multilingual-cased' version, configured for sequence classification tasks. This model is designed to handle complex natural language processing challenges, adept at understanding and analyzing text in multiple languages. We specifically used this model for classifying text into six categories, indicated by the num_labels=6 parameter. This setup allows the model to learn and predict various levels of text difficulty, harnessing BERT's sophisticated understanding of language context and structure.\n",
    "\n",
    "BERT's architecture, combined with the sequence classification layer, provided a powerful tool for our text classification needs. The model's ability to process and analyze textual data in depth, considering the contextual nuances of language, made it an ideal choice for accurately categorizing texts into predefined difficulty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/training_data.csv\").dropna()\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/unlabelled_test_data.csv\").dropna()\n",
    "df_final = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/sample_submission.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "training_data = df_train\n",
    "unlabelled_test_data = df_test\n",
    "\n",
    "class FrenchTextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "encoded_training_data = tokenizer(training_data['sentence'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "difficulty_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
    "training_labels = training_data['difficulty'].map(difficulty_mapping).tolist()\n",
    "\n",
    "train_dataset = FrenchTextDataset(encoded_training_data, training_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=6)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3): \n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(torch.int64)\n",
    "        attention_mask = batch['attention_mask'].to(torch.int64)\n",
    "        labels = batch['labels'].to(torch.int64)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "encoded_test_data = tokenizer(unlabelled_test_data['sentence'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_dataset = FrenchTextDataset(encoded_test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predicted_difficulties = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(torch.int64)\n",
    "        attention_mask = batch['attention_mask'].to(torch.int64)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predicted_difficulties.extend(predictions)\n",
    "\n",
    "reverse_difficulty_mapping = {v: k for k, v in difficulty_mapping.items()}\n",
    "predicted_difficulties = [reverse_difficulty_mapping[label] for label in predicted_difficulties]\n",
    "\n",
    "result_df = pd.DataFrame({'id': unlabelled_test_data['id'], 'difficulty': predicted_difficulties})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
