{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1a8sYlJ2SO7MIdelmYEqcXgXF5PtA1F6p#scrollTo=Nyv_XJIYmb10\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AkshatSurolia**\n",
    "\n",
    "We utilized a specialized model from Hugging Face's extensive model hub, specifically the 'AkshatSurolia/ICD-10-Code-Prediction' pre-trained model. This model is based on the BERT architecture, which has revolutionized the field of natural language processing due to its deep understanding of context and nuance in language. Originating from a repository known for its wide array of state-of-the-art machine learning models, this particular model was initially trained to predict ICD-10 medical codes, showcasing its ability to handle complex, specialized language tasks.\n",
    "\n",
    "Adapting this model to our specific need, which was classifying texts into different difficulty levels, we harnessed its advanced capabilities in processing and understanding language. The model's pre-trained foundation provided a robust starting point, allowing us to fine-tune it on our dataset for accurate difficulty classification. This approach exemplifies the power of using advanced pre-trained models to efficiently tackle specialized tasks like text classification, demonstrating how these models can be repurposed beyond their initial training objectives to suit a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/training_data.csv\").dropna()\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/unlabelled_test_data.csv\").dropna()\n",
    "df_final = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/sample_submission.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_train['encoded_labels'] = label_encoder.fit_transform(df_train['difficulty'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "\n",
    "train_encodings = tokenizer(df_train['sentence'].tolist(), truncation=True, padding=True, max_length=64)\n",
    "test_encodings = tokenizer(df_test['sentence'].tolist(), truncation=True, padding=True, max_length=64)\n",
    "\n",
    "class DifficultyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = DifficultyDataset(train_encodings, df_train['encoded_labels'].tolist())\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_dataset = DifficultyDataset(test_encodings)\n",
    "\n",
    "predictions = trainer.predict(test_dataset).predictions.argmax(-1)\n",
    "predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "df_final['difficulty'] = predicted_labels\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
