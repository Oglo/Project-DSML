{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1qgjp0y5sGEg9jjNKjMiaN6Y8Gsyzjodz#scrollTo=JvesfwSskZMq\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Machine (SVM)**\n",
    "\n",
    "For our next try we chose to use the Support Vector Machine (SVM) model, known for its efficiency in handling complex classification tasks. SVM works well with high-dimensional data, making it suitable for our needs, especially after preprocessing our text data through steps like tokenization, removing stopwords, and lemmatization. \n",
    "\n",
    "While SVM is powerful, it requires careful tuning of parameters to effectively manage the complexities of language data, a challenge we encountered in striving for optimal accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/training_data.csv\").dropna()\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/unlabelled_test_data.csv\").dropna()\n",
    "df_final = pd.read_csv(\"https://raw.githubusercontent.com/Oglo/Project-DSML/main/Data/sample_submission.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ]', ' ', text)\n",
    "    words = word_tokenize(text, language='french')\n",
    "    words = [word for word in words if word not in stopwords.words('french')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df_train['processed_sentence'] = df_train['sentence'].apply(preprocess_text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df_train['processed_sentence'])\n",
    "y = df_train['difficulty']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
